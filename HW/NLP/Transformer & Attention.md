# Transformer 理解
参考：
- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
- https://jalammar.github.io/illustrated-transformer/
## Seq2seq Models With Attention
1. sequence --> *encoder* --> **context**(vector of floats, 大小一般是256,512,1024) --> *decoder* -->sequence
2.  
